\doxysection{numpy.\+core.\+einsumfunc Namespace Reference}
\hypertarget{namespacenumpy_1_1core_1_1einsumfunc}{}\label{namespacenumpy_1_1core_1_1einsumfunc}\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a1df64765a8d91c52468bd6a72dfdb5eb}{\+\_\+flop\+\_\+count}} (idx\+\_\+contraction, inner, num\+\_\+terms, size\+\_\+dictionary)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a567812263ec3ccd69fd172d3b7a620e7}{\+\_\+compute\+\_\+size\+\_\+by\+\_\+dict}} (\mbox{\hyperlink{namespacenumpy_1_1core_1_1numeric_a15cc0d5f58abbccc68308d3e9950c281}{indices}}, idx\+\_\+dict)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_aa6015789b505cf3c5bc38fa2a44ecc13}{\+\_\+find\+\_\+contraction}} (positions, input\+\_\+sets, output\+\_\+set)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a2b1feaa0c4341e938a7008c8def47116}{\+\_\+optimal\+\_\+path}} (input\+\_\+sets, output\+\_\+set, idx\+\_\+dict, memory\+\_\+limit)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a8c76b6ec816c14879cb44afc4cafd727}{\+\_\+parse\+\_\+possible\+\_\+contraction}} (positions, input\+\_\+sets, output\+\_\+set, idx\+\_\+dict, memory\+\_\+limit, path\+\_\+cost, naive\+\_\+cost)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a29de8c077a127cff874f4f88c86d502f}{\+\_\+update\+\_\+other\+\_\+results}} (results, best)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a0fdba8ca26a525802671ef0fd3fa3eff}{\+\_\+greedy\+\_\+path}} (input\+\_\+sets, output\+\_\+set, idx\+\_\+dict, memory\+\_\+limit)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_ad5766493173dc597495dfc0a096889f5}{\+\_\+can\+\_\+dot}} (inputs, result, idx\+\_\+removed)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a4e423741b1db195dc46b134f0fa64974}{\+\_\+parse\+\_\+einsum\+\_\+input}} (operands)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a49b82b20560a9c6189449e6caf2e3709}{einsum\+\_\+path}} (\texorpdfstring{$\ast$}{*}operands, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}kwargs)
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a61a2025e88c965e34d9bc30b7dbc163a}{einsum}} (\texorpdfstring{$\ast$}{*}operands, \texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}kwargs)
\end{DoxyCompactItemize}
\doxysubsubsection*{Variables}
\begin{DoxyCompactItemize}
\item 
str \mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a14faa10d5aab0413463e3d321f3bda5b}{einsum\+\_\+symbols}} = \textquotesingle{}abcdefghijklmnopqrstuvwxyz\+ABCDEFGHIJKLMNOPQRSTUVWXYZ\textquotesingle{}
\item 
\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a4e709210a8cd851701c9cd136d487782}{einsum\+\_\+symbols\+\_\+set}} = set(\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a14faa10d5aab0413463e3d321f3bda5b}{einsum\+\_\+symbols}})
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Implementation of optimized einsum.\end{DoxyVerb}
 

\doxysubsection{Function Documentation}
\Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_ad5766493173dc597495dfc0a096889f5}\label{namespacenumpy_1_1core_1_1einsumfunc_ad5766493173dc597495dfc0a096889f5} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_can\_dot@{\_can\_dot}}
\index{\_can\_dot@{\_can\_dot}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_can\_dot()}{\_can\_dot()}}
{\footnotesize\ttfamily \+\_\+can\+\_\+dot (\begin{DoxyParamCaption}\item[{}]{inputs,  }\item[{}]{result,  }\item[{}]{idx\+\_\+removed }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Checks if we can use BLAS (np.tensordot) call and its beneficial to do so.

Parameters
----------
inputs : list of str
    Specifies the subscripts for summation.
result : str
    Resulting summation.
idx_removed : set
    Indices that are removed in the summation


Returns
-------
type : bool
    Returns true if BLAS should and can be used, else False

Notes
-----
If the operations is BLAS level 1 or 2 and is not already aligned
we default back to einsum as the memory movement to copy is more
costly than the operation itself.


Examples
--------

# Standard GEMM operation
>>> _can_dot(['ij', 'jk'], 'ik', set('j'))
True

# Can use the standard BLAS, but requires odd data movement
>>> _can_dot(['ijj', 'jk'], 'ik', set('j'))
False

# DDOT where the memory is not aligned
>>> _can_dot(['ijk', 'ikj'], '', set('ijk'))
False\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a567812263ec3ccd69fd172d3b7a620e7}\label{namespacenumpy_1_1core_1_1einsumfunc_a567812263ec3ccd69fd172d3b7a620e7} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_compute\_size\_by\_dict@{\_compute\_size\_by\_dict}}
\index{\_compute\_size\_by\_dict@{\_compute\_size\_by\_dict}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_compute\_size\_by\_dict()}{\_compute\_size\_by\_dict()}}
{\footnotesize\ttfamily \+\_\+compute\+\_\+size\+\_\+by\+\_\+dict (\begin{DoxyParamCaption}\item[{}]{indices,  }\item[{}]{idx\+\_\+dict }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes the product of the elements in indices based on the dictionary
idx_dict.

Parameters
----------
indices : iterable
    Indices to base the product on.
idx_dict : dictionary
    Dictionary of index sizes

Returns
-------
ret : int
    The resulting product.

Examples
--------
>>> _compute_size_by_dict('abbc', {'a': 2, 'b':3, 'c':5})
90\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_aa6015789b505cf3c5bc38fa2a44ecc13}\label{namespacenumpy_1_1core_1_1einsumfunc_aa6015789b505cf3c5bc38fa2a44ecc13} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_find\_contraction@{\_find\_contraction}}
\index{\_find\_contraction@{\_find\_contraction}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_find\_contraction()}{\_find\_contraction()}}
{\footnotesize\ttfamily \+\_\+find\+\_\+contraction (\begin{DoxyParamCaption}\item[{}]{positions,  }\item[{}]{input\+\_\+sets,  }\item[{}]{output\+\_\+set }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Finds the contraction for a given set of input and output sets.

Parameters
----------
positions : iterable
    Integer positions of terms used in the contraction.
input_sets : list
    List of sets that represent the lhs side of the einsum subscript
output_set : set
    Set that represents the rhs side of the overall einsum subscript

Returns
-------
new_result : set
    The indices of the resulting contraction
remaining : list
    List of sets that have not been contracted, the new set is appended to
    the end of this list
idx_removed : set
    Indices removed from the entire contraction
idx_contraction : set
    The indices used in the current contraction

Examples
--------

# A simple dot product test case
>>> pos = (0, 1)
>>> isets = [set('ab'), set('bc')]
>>> oset = set('ac')
>>> _find_contraction(pos, isets, oset)
({'a', 'c'}, [{'a', 'c'}], {'b'}, {'a', 'b', 'c'})

# A more complex case with additional terms in the contraction
>>> pos = (0, 2)
>>> isets = [set('abd'), set('ac'), set('bdc')]
>>> oset = set('ac')
>>> _find_contraction(pos, isets, oset)
({'a', 'c'}, [{'a', 'c'}, {'a', 'c'}], {'b', 'd'}, {'a', 'b', 'c', 'd'})
\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a1df64765a8d91c52468bd6a72dfdb5eb}\label{namespacenumpy_1_1core_1_1einsumfunc_a1df64765a8d91c52468bd6a72dfdb5eb} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_flop\_count@{\_flop\_count}}
\index{\_flop\_count@{\_flop\_count}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_flop\_count()}{\_flop\_count()}}
{\footnotesize\ttfamily \+\_\+flop\+\_\+count (\begin{DoxyParamCaption}\item[{}]{idx\+\_\+contraction,  }\item[{}]{inner,  }\item[{}]{num\+\_\+terms,  }\item[{}]{size\+\_\+dictionary }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes the number of FLOPS in the contraction.

Parameters
----------
idx_contraction : iterable
    The indices involved in the contraction
inner : bool
    Does this contraction require an inner product?
num_terms : int
    The number of terms in a contraction
size_dictionary : dict
    The size of each of the indices in idx_contraction

Returns
-------
flop_count : int
    The total number of FLOPS required for the contraction.

Examples
--------

>>> _flop_count('abc', False, 1, {'a': 2, 'b':3, 'c':5})
90

>>> _flop_count('abc', True, 2, {'a': 2, 'b':3, 'c':5})
270\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a0fdba8ca26a525802671ef0fd3fa3eff}\label{namespacenumpy_1_1core_1_1einsumfunc_a0fdba8ca26a525802671ef0fd3fa3eff} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_greedy\_path@{\_greedy\_path}}
\index{\_greedy\_path@{\_greedy\_path}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_greedy\_path()}{\_greedy\_path()}}
{\footnotesize\ttfamily \+\_\+greedy\+\_\+path (\begin{DoxyParamCaption}\item[{}]{input\+\_\+sets,  }\item[{}]{output\+\_\+set,  }\item[{}]{idx\+\_\+dict,  }\item[{}]{memory\+\_\+limit }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Finds the path by contracting the best pair until the input list is
exhausted. The best pair is found by minimizing the tuple
``(-prod(indices_removed), cost)``.  What this amounts to is prioritizing
matrix multiplication or inner product operations, then Hadamard like
operations, and finally outer operations. Outer products are limited by
``memory_limit``. This algorithm scales cubically with respect to the
number of elements in the list ``input_sets``.

Parameters
----------
input_sets : list
    List of sets that represent the lhs side of the einsum subscript
output_set : set
    Set that represents the rhs side of the overall einsum subscript
idx_dict : dictionary
    Dictionary of index sizes
memory_limit_limit : int
    The maximum number of elements in a temporary array

Returns
-------
path : list
    The greedy contraction order within the memory limit constraint.

Examples
--------
>>> isets = [set('abd'), set('ac'), set('bdc')]
>>> oset = set('')
>>> idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}
>>> _path__greedy_path(isets, oset, idx_sizes, 5000)
[(0, 2), (0, 1)]
\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a2b1feaa0c4341e938a7008c8def47116}\label{namespacenumpy_1_1core_1_1einsumfunc_a2b1feaa0c4341e938a7008c8def47116} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_optimal\_path@{\_optimal\_path}}
\index{\_optimal\_path@{\_optimal\_path}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_optimal\_path()}{\_optimal\_path()}}
{\footnotesize\ttfamily \+\_\+optimal\+\_\+path (\begin{DoxyParamCaption}\item[{}]{input\+\_\+sets,  }\item[{}]{output\+\_\+set,  }\item[{}]{idx\+\_\+dict,  }\item[{}]{memory\+\_\+limit }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Computes all possible pair contractions, sieves the results based
on ``memory_limit`` and returns the lowest cost path. This algorithm
scales factorial with respect to the elements in the list ``input_sets``.

Parameters
----------
input_sets : list
    List of sets that represent the lhs side of the einsum subscript
output_set : set
    Set that represents the rhs side of the overall einsum subscript
idx_dict : dictionary
    Dictionary of index sizes
memory_limit : int
    The maximum number of elements in a temporary array

Returns
-------
path : list
    The optimal contraction order within the memory limit constraint.

Examples
--------
>>> isets = [set('abd'), set('ac'), set('bdc')]
>>> oset = set('')
>>> idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}
>>> _path__optimal_path(isets, oset, idx_sizes, 5000)
[(0, 2), (0, 1)]
\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a4e423741b1db195dc46b134f0fa64974}\label{namespacenumpy_1_1core_1_1einsumfunc_a4e423741b1db195dc46b134f0fa64974} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_parse\_einsum\_input@{\_parse\_einsum\_input}}
\index{\_parse\_einsum\_input@{\_parse\_einsum\_input}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_parse\_einsum\_input()}{\_parse\_einsum\_input()}}
{\footnotesize\ttfamily \+\_\+parse\+\_\+einsum\+\_\+input (\begin{DoxyParamCaption}\item[{}]{operands }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}A reproduction of einsum c side einsum parsing in python.

Returns
-------
input_strings : str
    Parsed input strings
output_string : str
    Parsed output string
operands : list of array_like
    The operands to use in the numpy contraction

Examples
--------
The operand list is simplified to reduce printing:

>>> a = np.random.rand(4, 4)
>>> b = np.random.rand(4, 4, 4)
>>> __parse_einsum_input(('...a,...a->...', a, b))
('za,xza', 'xz', [a, b])

>>> __parse_einsum_input((a, [Ellipsis, 0], b, [Ellipsis, 0]))
('za,xza', 'xz', [a, b])
\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a8c76b6ec816c14879cb44afc4cafd727}\label{namespacenumpy_1_1core_1_1einsumfunc_a8c76b6ec816c14879cb44afc4cafd727} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_parse\_possible\_contraction@{\_parse\_possible\_contraction}}
\index{\_parse\_possible\_contraction@{\_parse\_possible\_contraction}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_parse\_possible\_contraction()}{\_parse\_possible\_contraction()}}
{\footnotesize\ttfamily \+\_\+parse\+\_\+possible\+\_\+contraction (\begin{DoxyParamCaption}\item[{}]{positions,  }\item[{}]{input\+\_\+sets,  }\item[{}]{output\+\_\+set,  }\item[{}]{idx\+\_\+dict,  }\item[{}]{memory\+\_\+limit,  }\item[{}]{path\+\_\+cost,  }\item[{}]{naive\+\_\+cost }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Compute the cost (removed size + flops) and resultant indices for
performing the contraction specified by ``positions``.

Parameters
----------
positions : tuple of int
    The locations of the proposed tensors to contract.
input_sets : list of sets
    The indices found on each tensors.
output_set : set
    The output indices of the expression.
idx_dict : dict
    Mapping of each index to its size.
memory_limit : int
    The total allowed size for an intermediary tensor.
path_cost : int
    The contraction cost so far.
naive_cost : int
    The cost of the unoptimized expression.

Returns
-------
cost : (int, int)
    A tuple containing the size of any indices removed, and the flop cost.
positions : tuple of int
    The locations of the proposed tensors to contract.
new_input_sets : list of sets
    The resulting new list of indices if this proposed contraction is performed.\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a29de8c077a127cff874f4f88c86d502f}\label{namespacenumpy_1_1core_1_1einsumfunc_a29de8c077a127cff874f4f88c86d502f} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!\_update\_other\_results@{\_update\_other\_results}}
\index{\_update\_other\_results@{\_update\_other\_results}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{\_update\_other\_results()}{\_update\_other\_results()}}
{\footnotesize\ttfamily \+\_\+update\+\_\+other\+\_\+results (\begin{DoxyParamCaption}\item[{}]{results,  }\item[{}]{best }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}}

\begin{DoxyVerb}Update the positions and provisional input_sets of ``results`` based on
performing the contraction result ``best``. Remove any involving the tensors
contracted.

Parameters
----------
results : list
    List of contraction results produced by ``_parse_possible_contraction``.
best : list
    The best contraction of ``results`` i.e. the one that will be performed.

Returns
-------
mod_results : list
    The list of modifed results, updated with outcome of ``best`` contraction.
\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a61a2025e88c965e34d9bc30b7dbc163a}\label{namespacenumpy_1_1core_1_1einsumfunc_a61a2025e88c965e34d9bc30b7dbc163a} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!einsum@{einsum}}
\index{einsum@{einsum}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{einsum()}{einsum()}}
{\footnotesize\ttfamily einsum (\begin{DoxyParamCaption}\item[{\texorpdfstring{$\ast$}{*}}]{operands,  }\item[{\texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{kwargs }\end{DoxyParamCaption})}

\begin{DoxyVerb}einsum(subscripts, *operands, out=None, dtype=None, order='K',
       casting='safe', optimize=False)

Evaluates the Einstein summation convention on the operands.

Using the Einstein summation convention, many common multi-dimensional
array operations can be represented in a simple fashion.  This function
provides a way to compute such summations. The best way to understand this
function is to try the examples below, which show how many common NumPy
functions can be implemented as calls to `einsum`.

Parameters
----------
subscripts : str
    Specifies the subscripts for summation.
operands : list of array_like
    These are the arrays for the operation.
out : {ndarray, None}, optional
    If provided, the calculation is done into this array.
dtype : {data-type, None}, optional
    If provided, forces the calculation to use the data type specified.
    Note that you may have to also give a more liberal `casting`
    parameter to allow the conversions. Default is None.
order : {'C', 'F', 'A', 'K'}, optional
    Controls the memory layout of the output. 'C' means it should
    be C contiguous. 'F' means it should be Fortran contiguous,
    'A' means it should be 'F' if the inputs are all 'F', 'C' otherwise.
    'K' means it should be as close to the layout as the inputs as
    is possible, including arbitrarily permuted axes.
    Default is 'K'.
casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
    Controls what kind of data casting may occur.  Setting this to
    'unsafe' is not recommended, as it can adversely affect accumulations.

      * 'no' means the data types should not be cast at all.
      * 'equiv' means only byte-order changes are allowed.
      * 'safe' means only casts which can preserve values are allowed.
      * 'same_kind' means only safe casts or casts within a kind,
        like float64 to float32, are allowed.
      * 'unsafe' means any data conversions may be done.

    Default is 'safe'.
optimize : {False, True, 'greedy', 'optimal'}, optional
    Controls if intermediate optimization should occur. No optimization
    will occur if False and True will default to the 'greedy' algorithm.
    Also accepts an explicit contraction list from the ``np.einsum_path``
    function. See ``np.einsum_path`` for more details. Defaults to False.

Returns
-------
output : ndarray
    The calculation based on the Einstein summation convention.

See Also
--------
einsum_path, dot, inner, outer, tensordot, linalg.multi_dot

Notes
-----
.. versionadded:: 1.6.0

The subscripts string is a comma-separated list of subscript labels,
where each label refers to a dimension of the corresponding operand.
Repeated subscripts labels in one operand take the diagonal.  For example,
``np.einsum('ii', a)`` is equivalent to ``np.trace(a)``.

Whenever a label is repeated, it is summed, so ``np.einsum('i,i', a, b)``
is equivalent to ``np.inner(a,b)``.  If a label appears only once,
it is not summed, so ``np.einsum('i', a)`` produces a view of ``a``
with no changes.

The order of labels in the output is by default alphabetical.  This
means that ``np.einsum('ij', a)`` doesn't affect a 2D array, while
``np.einsum('ji', a)`` takes its transpose.

The output can be controlled by specifying output subscript labels
as well.  This specifies the label order, and allows summing to
be disallowed or forced when desired.  The call ``np.einsum('i->', a)``
is like ``np.sum(a, axis=-1)``, and ``np.einsum('ii->i', a)``
is like ``np.diag(a)``.  The difference is that `einsum` does not
allow broadcasting by default.

To enable and control broadcasting, use an ellipsis.  Default
NumPy-style broadcasting is done by adding an ellipsis
to the left of each term, like ``np.einsum('...ii->...i', a)``.
To take the trace along the first and last axes,
you can do ``np.einsum('i...i', a)``, or to do a matrix-matrix
product with the left-most indices instead of rightmost, you can do
``np.einsum('ij...,jk...->ik...', a, b)``.

When there is only one operand, no axes are summed, and no output
parameter is provided, a view into the operand is returned instead
of a new array.  Thus, taking the diagonal as ``np.einsum('ii->i', a)``
produces a view.

An alternative way to provide the subscripts and operands is as
``einsum(op0, sublist0, op1, sublist1, ..., [sublistout])``. The examples
below have corresponding `einsum` calls with the two parameter methods.

.. versionadded:: 1.10.0

Views returned from einsum are now writeable whenever the input array
is writeable. For example, ``np.einsum('ijk...->kji...', a)`` will now
have the same effect as ``np.swapaxes(a, 0, 2)`` and
``np.einsum('ii->i', a)`` will return a writeable view of the diagonal
of a 2D array.

.. versionadded:: 1.12.0

Added the ``optimize`` argument which will optimize the contraction order
of an einsum expression. For a contraction with three or more operands this
can greatly increase the computational efficiency at the cost of a larger
memory footprint during computation.

See ``np.einsum_path`` for more details.

Examples
--------
>>> a = np.arange(25).reshape(5,5)
>>> b = np.arange(5)
>>> c = np.arange(6).reshape(2,3)

>>> np.einsum('ii', a)
60
>>> np.einsum(a, [0,0])
60
>>> np.trace(a)
60

>>> np.einsum('ii->i', a)
array([ 0,  6, 12, 18, 24])
>>> np.einsum(a, [0,0], [0])
array([ 0,  6, 12, 18, 24])
>>> np.diag(a)
array([ 0,  6, 12, 18, 24])

>>> np.einsum('ij,j', a, b)
array([ 30,  80, 130, 180, 230])
>>> np.einsum(a, [0,1], b, [1])
array([ 30,  80, 130, 180, 230])
>>> np.dot(a, b)
array([ 30,  80, 130, 180, 230])
>>> np.einsum('...j,j', a, b)
array([ 30,  80, 130, 180, 230])

>>> np.einsum('ji', c)
array([[0, 3],
       [1, 4],
       [2, 5]])
>>> np.einsum(c, [1,0])
array([[0, 3],
       [1, 4],
       [2, 5]])
>>> c.T
array([[0, 3],
       [1, 4],
       [2, 5]])

>>> np.einsum('..., ...', 3, c)
array([[ 0,  3,  6],
       [ 9, 12, 15]])
>>> np.einsum(',ij', 3, C)
array([[ 0,  3,  6],
       [ 9, 12, 15]])
>>> np.einsum(3, [Ellipsis], c, [Ellipsis])
array([[ 0,  3,  6],
       [ 9, 12, 15]])
>>> np.multiply(3, c)
array([[ 0,  3,  6],
       [ 9, 12, 15]])

>>> np.einsum('i,i', b, b)
30
>>> np.einsum(b, [0], b, [0])
30
>>> np.inner(b,b)
30

>>> np.einsum('i,j', np.arange(2)+1, b)
array([[0, 1, 2, 3, 4],
       [0, 2, 4, 6, 8]])
>>> np.einsum(np.arange(2)+1, [0], b, [1])
array([[0, 1, 2, 3, 4],
       [0, 2, 4, 6, 8]])
>>> np.outer(np.arange(2)+1, b)
array([[0, 1, 2, 3, 4],
       [0, 2, 4, 6, 8]])

>>> np.einsum('i...->...', a)
array([50, 55, 60, 65, 70])
>>> np.einsum(a, [0,Ellipsis], [Ellipsis])
array([50, 55, 60, 65, 70])
>>> np.sum(a, axis=0)
array([50, 55, 60, 65, 70])

>>> a = np.arange(60.).reshape(3,4,5)
>>> b = np.arange(24.).reshape(4,3,2)
>>> np.einsum('ijk,jil->kl', a, b)
array([[ 4400.,  4730.],
       [ 4532.,  4874.],
       [ 4664.,  5018.],
       [ 4796.,  5162.],
       [ 4928.,  5306.]])
>>> np.einsum(a, [0,1,2], b, [1,0,3], [2,3])
array([[ 4400.,  4730.],
       [ 4532.,  4874.],
       [ 4664.,  5018.],
       [ 4796.,  5162.],
       [ 4928.,  5306.]])
>>> np.tensordot(a,b, axes=([1,0],[0,1]))
array([[ 4400.,  4730.],
       [ 4532.,  4874.],
       [ 4664.,  5018.],
       [ 4796.,  5162.],
       [ 4928.,  5306.]])

>>> a = np.arange(6).reshape((3,2))
>>> b = np.arange(12).reshape((4,3))
>>> np.einsum('ki,jk->ij', a, b)
array([[10, 28, 46, 64],
       [13, 40, 67, 94]])
>>> np.einsum('ki,...k->i...', a, b)
array([[10, 28, 46, 64],
       [13, 40, 67, 94]])
>>> np.einsum('k...,jk', a, b)
array([[10, 28, 46, 64],
       [13, 40, 67, 94]])

>>> # since version 1.10.0
>>> a = np.zeros((3, 3))
>>> np.einsum('ii->i', a)[:] = 1
>>> a
array([[ 1.,  0.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  0.,  1.]])\end{DoxyVerb}
 \Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a49b82b20560a9c6189449e6caf2e3709}\label{namespacenumpy_1_1core_1_1einsumfunc_a49b82b20560a9c6189449e6caf2e3709} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!einsum\_path@{einsum\_path}}
\index{einsum\_path@{einsum\_path}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{einsum\_path()}{einsum\_path()}}
{\footnotesize\ttfamily einsum\+\_\+path (\begin{DoxyParamCaption}\item[{\texorpdfstring{$\ast$}{*}}]{operands,  }\item[{\texorpdfstring{$\ast$}{*}\texorpdfstring{$\ast$}{*}}]{kwargs }\end{DoxyParamCaption})}

\begin{DoxyVerb}einsum_path(subscripts, *operands, optimize='greedy')

Evaluates the lowest cost contraction order for an einsum expression by
considering the creation of intermediate arrays.

Parameters
----------
subscripts : str
    Specifies the subscripts for summation.
*operands : list of array_like
    These are the arrays for the operation.
optimize : {bool, list, tuple, 'greedy', 'optimal'}
    Choose the type of path. If a tuple is provided, the second argument is
    assumed to be the maximum intermediate size created. If only a single
    argument is provided the largest input or output array size is used
    as a maximum intermediate size.

    * if a list is given that starts with ``einsum_path``, uses this as the
      contraction path
    * if False no optimization is taken
    * if True defaults to the 'greedy' algorithm
    * 'optimal' An algorithm that combinatorially explores all possible
      ways of contracting the listed tensors and choosest the least costly
      path. Scales exponentially with the number of terms in the
      contraction.
    * 'greedy' An algorithm that chooses the best pair contraction
      at each step. Effectively, this algorithm searches the largest inner,
      Hadamard, and then outer products at each step. Scales cubically with
      the number of terms in the contraction. Equivalent to the 'optimal'
      path for most contractions.

    Default is 'greedy'.

Returns
-------
path : list of tuples
    A list representation of the einsum path.
string_repr : str
    A printable representation of the einsum path.

Notes
-----
The resulting path indicates which terms of the input contraction should be
contracted first, the result of this contraction is then appended to the
end of the contraction list. This list can then be iterated over until all
intermediate contractions are complete.

See Also
--------
einsum, linalg.multi_dot

Examples
--------

We can begin with a chain dot example. In this case, it is optimal to
contract the ``b`` and ``c`` tensors first as represented by the first
element of the path ``(1, 2)``. The resulting tensor is added to the end
of the contraction and the remaining contraction ``(0, 1)`` is then
completed.

>>> a = np.random.rand(2, 2)
>>> b = np.random.rand(2, 5)
>>> c = np.random.rand(5, 2)
>>> path_info = np.einsum_path('ij,jk,kl->il', a, b, c, optimize='greedy')
>>> print(path_info[0])
['einsum_path', (1, 2), (0, 1)]
>>> print(path_info[1])
  Complete contraction:  ij,jk,kl->il
         Naive scaling:  4
     Optimized scaling:  3
      Naive FLOP count:  1.600e+02
  Optimized FLOP count:  5.600e+01
   Theoretical speedup:  2.857
  Largest intermediate:  4.000e+00 elements
-------------------------------------------------------------------------
scaling                  current                                remaining
-------------------------------------------------------------------------
   3                   kl,jk->jl                                ij,jl->il
   3                   jl,ij->il                                   il->il


A more complex index transformation example.

>>> I = np.random.rand(10, 10, 10, 10)
>>> C = np.random.rand(10, 10)
>>> path_info = np.einsum_path('ea,fb,abcd,gc,hd->efgh', C, C, I, C, C,
                               optimize='greedy')

>>> print(path_info[0])
['einsum_path', (0, 2), (0, 3), (0, 2), (0, 1)]
>>> print(path_info[1])
  Complete contraction:  ea,fb,abcd,gc,hd->efgh
         Naive scaling:  8
     Optimized scaling:  5
      Naive FLOP count:  8.000e+08
  Optimized FLOP count:  8.000e+05
   Theoretical speedup:  1000.000
  Largest intermediate:  1.000e+04 elements
--------------------------------------------------------------------------
scaling                  current                                remaining
--------------------------------------------------------------------------
   5               abcd,ea->bcde                      fb,gc,hd,bcde->efgh
   5               bcde,fb->cdef                         gc,hd,cdef->efgh
   5               cdef,gc->defg                            hd,defg->efgh
   5               defg,hd->efgh                               efgh->efgh
\end{DoxyVerb}
 

\doxysubsection{Variable Documentation}
\Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a14faa10d5aab0413463e3d321f3bda5b}\label{namespacenumpy_1_1core_1_1einsumfunc_a14faa10d5aab0413463e3d321f3bda5b} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!einsum\_symbols@{einsum\_symbols}}
\index{einsum\_symbols@{einsum\_symbols}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{einsum\_symbols}{einsum\_symbols}}
{\footnotesize\ttfamily str einsum\+\_\+symbols = \textquotesingle{}abcdefghijklmnopqrstuvwxyz\+ABCDEFGHIJKLMNOPQRSTUVWXYZ\textquotesingle{}}

\Hypertarget{namespacenumpy_1_1core_1_1einsumfunc_a4e709210a8cd851701c9cd136d487782}\label{namespacenumpy_1_1core_1_1einsumfunc_a4e709210a8cd851701c9cd136d487782} 
\index{numpy.core.einsumfunc@{numpy.core.einsumfunc}!einsum\_symbols\_set@{einsum\_symbols\_set}}
\index{einsum\_symbols\_set@{einsum\_symbols\_set}!numpy.core.einsumfunc@{numpy.core.einsumfunc}}
\doxysubsubsection{\texorpdfstring{einsum\_symbols\_set}{einsum\_symbols\_set}}
{\footnotesize\ttfamily einsum\+\_\+symbols\+\_\+set = set(\mbox{\hyperlink{namespacenumpy_1_1core_1_1einsumfunc_a14faa10d5aab0413463e3d321f3bda5b}{einsum\+\_\+symbols}})}

